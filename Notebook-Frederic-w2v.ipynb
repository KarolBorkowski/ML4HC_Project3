{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding model - training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v hyperparameters: \n",
    "vec_size = 100\n",
    "window = 7\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preprocessing train data...', end=' ', flush=True)\n",
    "labels_train, corpus_train = get_data('train')\n",
    "labels_valid, corpus_valid = get_data('dev')\n",
    "labels_test, corpus_test = get_data('test')\n",
    "print('done.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training w2v model...', end=' ', flush=True)\n",
    "    model = Word2Vec(sentences=corpus_train, vector_size=vec_size,\n",
    "                     window=window, min_count=1, workers=4, epochs=epochs)\n",
    "    model.save(\n",
    "        \"trained_models/word2vec_{}_{}_{}.model\".format(vec_size, window, epochs))\n",
    "    print('done.', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding model - evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose the hyperparameters for the w2v model based on the following evaluation, which yielded the parameters above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the eval metrics from word2vec word pairs: \n",
    "word_sim = model.wv.evaluate_word_pairs(datapath('wordsim353.tsv'))\n",
    "print('output from word pairs: ')\n",
    "print(word_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with word2vec embedding - preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to preprocess the data by embedding the words and averaging them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map text to vectors and average each sentence to one vector:\n",
    "all_vector_texts = []\n",
    "all_labels = []\n",
    "not_in_model = []\n",
    "for text, labels in zip([corpus_train, corpus_valid, corpus_test], [labels_train, labels_valid,labels_test]):\n",
    "    delete_labels = []\n",
    "    vector_text = []\n",
    "    for i,sentence in enumerate(text):\n",
    "        # assert that sentence is not empty:\n",
    "        if sentence == []:\n",
    "            delete_labels.append(i)\n",
    "            continue\n",
    "\n",
    "        feature_vec = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                feature_vec.append(w2v_model.wv[word])\n",
    "            except Exception:\n",
    "                not_in_model.append(word)\n",
    "\n",
    "        mean = np.array(feature_vec).mean(axis=0)\n",
    "        # also get rid of nan means due to only unknown words (example: sentence with 1 word not in dict)\n",
    "        if np.shape(mean) != (vec_size,):\n",
    "            delete_labels.append(i)\n",
    "            continue\n",
    "\n",
    "        vector_text.append(mean)\n",
    "    # delete labels:\n",
    "    for i in sorted(delete_labels, reverse=True):\n",
    "        del labels[i]\n",
    "\n",
    "    all_vector_texts.append(vector_text)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "train_vector_text = all_vector_texts[0]\n",
    "val_vector_text = all_vector_texts[1]\n",
    "test_vector_text = all_vector_texts[2]\n",
    "\n",
    "train_labels = all_labels[0]\n",
    "val_labels = all_labels[1]\n",
    "test_labels = all_labels[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with word2vec embedding - Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out logistic regression on top of the averaged embeddings. We did a grid search to find the parameters that worked best. (l2 penalty , C=5, class weigths = None, F1_weighted score for val and test respectively: 0.746, 0.749) Sadly we did not manage to yield better result here, than on the baseline. \n",
    "\n",
    "We also tried varying the word2vec models here, finding that vec_size=300 worked best here, although it performed worse on the word pairs task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000)\n",
    "log_reg.fit(np.array(train_vector_text), np.array(train_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on val and test: \n",
    "def evaluate(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    micro = f1_score(y, y_pred, average='micro')\n",
    "    macro = f1_score(y, y_pred, average='macro')\n",
    "    weighted = f1_score(y, y_pred, average='weighted')\n",
    "    # samples = f1_score(y, y_pred, average='samples')\n",
    "    print(f'F1 Score: micro {micro}, macro {macro}, weighted {weighted}')\n",
    "\n",
    "evaluate(log_reg, val_vector_text, val_labels)\n",
    "evaluate(log_reg, test_vector_text, test_labels)\n",
    "\n",
    "# save the model\n",
    "model_name_logreg = 'log_reg.sav'\n",
    "pickle.dump(log_reg, open(model_name_logreg, 'wb'))\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
