{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding model - training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v hyperparameters: \n",
    "vec_size = 100\n",
    "window = 7\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing train data... "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5b41040e025d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Preprocessing train data...'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlabels_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dev'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'done.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_data' is not defined"
     ]
    }
   ],
   "source": [
    "print('Preprocessing train data...', end=' ', flush=True)\n",
    "labels_train, corpus_train = get_data('train')\n",
    "labels_valid, corpus_valid = get_data('dev')\n",
    "labels_test, corpus_test = get_data('test')\n",
    "print('done.', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-3-7e5ab24b6570>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-7e5ab24b6570>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    model = Word2Vec(sentences=corpus_train, vector_size=vec_size,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "print('Training w2v model...', end=' ', flush=True)\n",
    "    model = Word2Vec(sentences=corpus_train, vector_size=vec_size,\n",
    "                     window=window, min_count=1, workers=4, epochs=epochs)\n",
    "    model.save(\n",
    "        \"trained_models/word2vec_{}_{}_{}.model\".format(vec_size, window, epochs))\n",
    "    print('done.', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding model - evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose the hyperparameters for the w2v model based on the following evaluation, which yielded the parameters above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a85923d9c6cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# take a look at the eval metrics from word2vec word pairs:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_word_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wordsim353.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output from word pairs: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_sim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# take a look at the eval metrics from word2vec word pairs: \n",
    "word_sim = model.wv.evaluate_word_pairs(datapath('wordsim353.tsv'))\n",
    "print('output from word pairs: ')\n",
    "print(word_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with word2vec embedding - preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to preprocess the data by embedding the words and averaging them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map text to vectors and average each sentence to one vector:\n",
    "all_vector_texts = []\n",
    "all_labels = []\n",
    "not_in_model = []\n",
    "for text, labels in zip([corpus_train, corpus_valid, corpus_test], [labels_train, labels_valid,labels_test]):\n",
    "    delete_labels = []\n",
    "    vector_text = []\n",
    "    for i,sentence in enumerate(text):\n",
    "        # assert that sentence is not empty:\n",
    "        if sentence == []:\n",
    "            delete_labels.append(i)\n",
    "            continue\n",
    "\n",
    "        feature_vec = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                feature_vec.append(w2v_model.wv[word])\n",
    "            except Exception:\n",
    "                not_in_model.append(word)\n",
    "\n",
    "        mean = np.array(feature_vec).mean(axis=0)\n",
    "        # also get rid of nan means due to only unknown words (example: sentence with 1 word not in dict)\n",
    "        if np.shape(mean) != (vec_size,):\n",
    "            delete_labels.append(i)\n",
    "            continue\n",
    "\n",
    "        vector_text.append(mean)\n",
    "    # delete labels:\n",
    "    for i in sorted(delete_labels, reverse=True):\n",
    "        del labels[i]\n",
    "\n",
    "    all_vector_texts.append(vector_text)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "train_vector_text = all_vector_texts[0]\n",
    "val_vector_text = all_vector_texts[1]\n",
    "test_vector_text = all_vector_texts[2]\n",
    "\n",
    "train_labels = all_labels[0]\n",
    "val_labels = all_labels[1]\n",
    "test_labels = all_labels[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with word2vec embedding - Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out logistic regression on top of the averaged embeddings. We did a grid search to find the parameters that worked best. (l2 penalty , C=5, class weigths = None, F1_weighted score for val and test respectively: 0.746, 0.749) Sadly we did not manage to yield better result here, than on the baseline. \n",
    "\n",
    "We also tried varying the word2vec models here, finding that vec_size=300 worked best here, although it performed worse on the word pairs task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000)\n",
    "log_reg.fit(np.array(train_vector_text), np.array(train_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on val and test: \n",
    "def evaluate(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    micro = f1_score(y, y_pred, average='micro')\n",
    "    macro = f1_score(y, y_pred, average='macro')\n",
    "    weighted = f1_score(y, y_pred, average='weighted')\n",
    "    # samples = f1_score(y, y_pred, average='samples')\n",
    "    print(f'F1 Score: micro {micro}, macro {macro}, weighted {weighted}')\n",
    "\n",
    "evaluate(log_reg, val_vector_text, val_labels)\n",
    "evaluate(log_reg, test_vector_text, test_labels)\n",
    "\n",
    "# save the model\n",
    "model_name_logreg = 'log_reg.sav'\n",
    "pickle.dump(log_reg, open(model_name_logreg, 'wb'))\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
